---
description: Manual UI/UX testing and backend validation on local dev before shipping
version: 2.0
updated: 2025-11-17
---

# /preview — Manual Testing & Validation

Preview feature: $ARGUMENTS

<context>
## MENTAL MODEL

**Workflow**: spec → clarify → plan → tasks → analyze → implement → optimize → **preview** → phase-1-ship → validate-staging → phase-2-ship

**What this does**:
- **UI Mode**: Generates testing checklist, starts dev servers, guides manual UI/UX testing, captures screenshots/performance
- **API Mode**: Contract diffs (OpenAPI), API smoke/property tests (Schemathesis/Newman), lightweight perf (k6), optional security scan (ZAP)
- **Data/Infra Mode**: Migration validation (Alembic upgrade/downgrade), worker dry-runs (Celery), seed/rollback tests
- Auto-detects modes from changed files and spec.md
- Documents issues for debugging
- Validates design implementation vs mockup

**State machine:**
- Load feature → Detect modes → Generate checklist → Start servers (UI/API/workers) → Interactive testing → Measure performance → Document results → Suggest next

**Auto-suggest:**
- If issues found → `/debug`
- If tests pass → `/phase-1-ship`

**Prerequisites**:
- `/optimize` must be complete
- Production routes or API endpoints implemented
- No critical blockers in optimization report
</context>

<instructions>
## USER INPUT

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Execute Preview Workflow

Run the centralized spec-cli tool:

```bash
python .spec-flow/scripts/spec-cli.py preview "$ARGUMENTS"
```

**What the script does:**

1. **Setup** — Cleanup handler, feature detection, prerequisite validation
2. **Affected apps** — Detects which apps have routes for this feature
3. **Mode detection** — Auto-detects UI/API/Data/Worker modes from changed files
4. **Generate checklist** — Extracts scenarios and acceptance criteria from spec.md
5. **Mockup comparison** — Adds design comparison section if polished mockups exist
6. **OpenAPI diff** — Compares current spec against baseline (if API mode)
7. **API tests** — Runs Schemathesis/Newman property-based tests (if API mode)
8. **Migration validation** — Tests upgrade/downgrade cycle (if Data mode)
9. **Performance check** — Lightweight k6 load test (if API mode)
10. **Security scan** — Optional ZAP baseline (if API mode)
11. **Start dev servers** — Launches affected apps on ports 3000/3001/3002
12. **Start backend** — Launches FastAPI/Celery if needed (8000/8010)
13. **Interactive testing** — Opens checklist, guides manual testing
14. **Mobile testing** — Optional physical device / emulation testing
15. **Accessibility** — Optional axe-core WCAG 2.1 AA scan
16. **Performance** — Optional Lighthouse desktop testing
17. **Visual testing** — Optional Playwright UI mode
18. **Screenshot capture** — Automated or manual screenshot collection
19. **Parse results** — Analyzes checklist completion, documents issues
20. **Git commit** — Commits preview checklist, screenshots, reports
21. **Suggest next** — Recommends `/debug` (if issues) or `/phase-1-ship` (if passed)

**Script output provides context for LLM:**

The script generates preview-checklist.md with:
- Routes to test
- User scenarios from spec.md
- Acceptance criteria
- Visual validation items
- Browser testing matrix
- Accessibility checklist
- Performance benchmarks
- API-specific items (if API mode)
- Design comparison (if mockups exist)

**After script completes, you (LLM) must:**

## 1) Review Checklist Completion

**Read checklist:**
- `specs/*/preview-checklist.md` (generated by script)

**Check for:**
- Incomplete testing (unchecked items)
- Documented issues (### Issue sections)
- Failed accessibility/performance checks
- Browser coverage gaps

## 2) Analyze Test Reports

**UI Mode artifacts:**
- `axe-*.json` (accessibility violations)
- `lighthouse-preview-*.json` (performance scores)
- `preview-screenshots/*.png` (visual evidence)

**API Mode artifacts:**
- `openapi-diff.txt` (breaking changes)
- `schemathesis-report.json` (property test failures)
- `newman-report.json` (Postman collection results)
- `k6-output.txt` (performance metrics)
- `zap-baseline.html` (security findings)

**Parse reports for:**
- Critical violations (accessibility, performance, security)
- Breaking changes (API contracts)
- Regression risks (visual differences vs mockup)

## 3) Determine Preview Status

**Status options:**
- `passed` — All tests complete, no blocking issues
- `issues_documented` — Minor issues found but acceptable
- `incomplete` — Testing not finished
- `failed` — Critical blockers found

**Decision logic:**
```
IF critical_issues > 0 THEN
  status = failed
  next_step = /debug
ELSE IF issues_found > 0 AND severity = minor THEN
  status = issues_documented
  next_step = /debug or /phase-1-ship (user choice)
ELSE IF completed_checks < total_checks THEN
  status = incomplete
  next_step = re-run /preview
ELSE
  status = passed
  next_step = /phase-1-ship
END IF
```

## 4) Present Results to User

**Summary format:**

```
Preview Summary

Feature: {slug}
Status: {passed|issues_documented|incomplete|failed}

Testing results:
  Scenarios tested: {completed} / {total}
  Issues found: {count}
  Browser coverage: {tested} / 6
  Routes tested: {count}

Artifacts:
  Checklist: specs/{slug}/preview-checklist.md
  Screenshots: {count}
  Lighthouse reports: {count}
  Axe reports: {count}
  [API-specific artifacts if applicable]

Next steps:
  {Recommended action based on status}
```

## 5) Suggest Next Action

**Based on status:**

**Passed:**
```
✅ Preview passed! Ready to ship to staging.

Next: /phase-1-ship

This will:
  1. Create pull request
  2. Enable auto-merge
  3. Wait for CI to pass
  4. Auto-merge to main
  5. Deploy to staging environment
```

**Issues documented:**
```
⚠️  Issues found but proceeding

Options:
  1. /debug - Fix issues first (recommended for critical issues)
  2. /phase-1-ship - Ship with known issues (minor issues only)

Documented issues:
  {List top 5 issues}
```

**Incomplete:**
```
⚠️  Testing incomplete

Options:
  1. Re-run /preview - Complete testing
  2. /phase-1-ship - Ship anyway (not recommended)
```

**Failed:**
```
❌ Critical issues found

Blocking issues:
  {List critical issues}

Next: /debug to fix blockers
      Then re-run /preview to verify
```

</instructions>

---

## NOTES

**Dev servers:**
- Marketing: http://localhost:3000
- App: http://localhost:3001
- Mock: http://localhost:3002 (if polished mockups exist)
- API: http://localhost:8000 (if backend exists)

**Cleanup:**
- Script automatically stops dev servers on exit
- PIDs saved to /tmp/preview-server-pids.txt
- Manual stop: `kill $(cat /tmp/preview-server-pids.txt)`

**Mobile testing:**
- Physical device: Connect to same network, use local IP
- Chrome DevTools: Device emulation (Cmd+Shift+M)
- Playwright: Mobile device configs (iPhone 13, Pixel 5)

**Accessibility:**
- Automated: axe-core WCAG 2.1 AA scan
- Manual: Keyboard nav, screen reader, focus indicators

**Performance:**
- Lighthouse: Desktop preset, performance/a11y/best-practices
- k6: 10 VUs, 30s duration (API mode)

**Security:**
- ZAP baseline scan (optional, requires Docker)
- OWASP API Security Top 10 checks
